{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-enhanced Speech Analytics Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature checklist\n",
    "- [ ] Speech length\n",
    "- [ ] Words per minute\n",
    "- [ ] Recognized text\n",
    "- [ ] Word count\n",
    "- [ ] Character count\n",
    "- [ ] Stemming/lemmatization\n",
    "- [ ] Popular words and stopwords\n",
    "- [ ] Sentiment analysis\n",
    "- [ ] Parts of speech tagging\n",
    "- [ ] Spelling correction\n",
    "- [ ] Tagalog language support\n",
    "\n",
    "## Challenges\n",
    "* Filipino language\n",
    "* Code switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemeting speech recognition\n",
    "Implementing speech recognition using a TedEd 3 minutes talk titled 'Try something new for 30 days' by Matt Cutts as audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the audio file to wav format and feeding it to Google speech recognition engine to get the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized text: a few years ago i felt like i was stuck in a rat so i decided to follow in the footsteps of the great american philosopher morgan spurlock and try something new for 30 days the idea is actually pretty simple think about something you've always wanted to add your life and try it for the next 30 days it turns out 30 days is just about the right amount of time to add a new habit or subtract the habit like watching the news from your life there's a few things that i learned while doing these 30 day challenges the first was instead of the months flying by forgotten the time was much more memorable this was part of a challenge i did to take a picture everyday for a month and i remember exactly where i was and what i was doing that day i also noticed that as i started to do more and harder 30 day challenges myself confidence grew i went from death dwelling computer nerd to the kind of guy who bikes to work for fun even last year i ended up hiking up mount kilimanjaro the highest mountain africa i would never have been that adventurous before i started my 30 day challenges i also figured out that if you really want something badly enough you can do anything for 30 days have you ever wondered a novel every november tense of thousands of people try to write their own 50,000 word novel from scratch in 30 days it turns out all you have to do is right 1667 words a day for a month so i did by the way the secret is not to go to sleep until you've written your words for the day you might be sleep deprived but you'll finish your novel now is my book the next great american novel no i wrote it in a month it's awful but for the rest of my life if i meet john hodgman at a ted party i don't have to say i'm a computer scientist no no if i want to i can say i'm a novelist so here's one last thing i like to mention i learned that when i made small sustainable changes things i could keep doing there were more likely to stick there's nothing wrong with big crazy challenges in fact they're a ton of fun butterless likely to stick when i gave up sugar for 30 days day 31 look like this so here's my question to you what are you waiting for i guarantee you the next 30 days are going to pass whether you like it or not so why not think about something you have always wanted to try and give it a shot for the next 30 days thanks\n"
     ]
    }
   ],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "try:\n",
    "  audio_full_filename = 'TedEd - Try something new for 30 days.mp3'\n",
    "  audio_filename = audio_full_filename[0:audio_full_filename.rfind('.')]\n",
    "  raw_audios_directory = 'audios/raw'\n",
    "  converted_audios_directory = 'audios/converted'\n",
    "\n",
    "  # Converting the audio file to wav format\n",
    "  audio_file = AudioSegment.from_file(f'{raw_audios_directory}/{audio_full_filename}')\n",
    "  test = audio_file.export(f'{converted_audios_directory}/{audio_filename}.wav', format='wav')\n",
    "\n",
    "  # Feeding the wav audio to Google speech recognition engine\n",
    "  with sr.AudioFile(f'{converted_audios_directory}/{audio_filename}.wav') as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    recognized_text = recognizer.recognize_google(audio_data)\n",
    "    print(f'Recognized text: {recognized_text}')\n",
    "\n",
    "except sr.UnknownValueError():\n",
    "  recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained model for sentence boundary recognition and punctuation restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "pipe = pipeline('token-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestorePunctuation:\n",
    "  _LABEL_MAP = {\n",
    "    \"LABEL_0\": \"OU\",\n",
    "    \"LABEL_1\": \"OO\",\n",
    "    \"LABEL_2\": \".O\",\n",
    "    \"LABEL_3\": \"!O\",\n",
    "    \"LABEL_4\": \",O\",\n",
    "    \"LABEL_5\": \".U\",\n",
    "    \"LABEL_6\": \"!U\",\n",
    "    \"LABEL_7\": \",U\",\n",
    "    \"LABEL_8\": \":O\",\n",
    "    \"LABEL_9\": \";O\",\n",
    "    \"LABEL_10\": \":U\",\n",
    "    \"LABEL_11\": \"'O\",\n",
    "    \"LABEL_12\": \"-O\",\n",
    "    \"LABEL_13\": \"?O\",\n",
    "    \"LABEL_14\": \"?U\",\n",
    "  }\n",
    "\n",
    "  def __init__(self):\n",
    "    self._tokenizer = AutoTokenizer.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "    self._model = AutoModelForTokenClassification.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "    self._pipe = pipeline('token-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "  def restore(self, text: str):\n",
    "    predictions = self._pipe(text)\n",
    "    \n",
    "    restored_text = ''\n",
    "    for token_prediction in predictions:\n",
    "      match  (self._LABEL_MAP[token_prediction['entity']]):\n",
    "        case (\"OU\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "        case (\"OO\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "        case (\".O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\".\")\n",
    "        case (\"!O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\"!\")\n",
    "        case (\",O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\",\")\n",
    "        case (\".U\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "          restored_text += (\".\")\n",
    "        case (\"!U\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "          restored_text += (\"!\")\n",
    "        case (\",U\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "          restored_text += (\",\")\n",
    "        case (\":O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\":\")\n",
    "        case (\";O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\";\")\n",
    "        case (\":U\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "          restored_text += (\":\")\n",
    "        case (\"'O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\"'\")\n",
    "        case (\"-O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\"-\")\n",
    "        case (\"?O\"):\n",
    "          restored_text += (token_prediction['word'])\n",
    "          restored_text += (\"!\")\n",
    "        case (\"?U\"):\n",
    "          restored_text += (token_prediction['word'].capitalize())\n",
    "          restored_text += (\"?\")\n",
    "      restored_text += ' '\n",
    "\n",
    "    restored_text = restored_text.replace(' ##', '').replace(\" ' \", \"'\")\n",
    "    return restored_text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A few years ago I felt like I was stuck in a rat. so I decided to follow in the footsteps of the great American philosopher Morgan Spurlock and try something new for 30 days. The idea is actually pretty simple. Think about something you've always wanted to add your life and try it for the next 30 days. It turns out 30 days is just about the right amount of time to add a new habit or subtract the habit. like watching the news from your life. There's a few things that I learned while doing these 30 day challenges. The first was instead of the months flying by forgotten, the time was much more memorable. This was part of a challenge I did to take a picture everyday for a month and I remember exactly where I was and what I was doing that day. I also noticed that as I started to do more and harder 30 day challenges myself, confidence grew. I went from death dwelling computer nerd to the kind of guy who bikes to work for fun. Even last year I ended up hiking up Mount Ki,limanjar,o, the highest mountain Africa. I would never have been that adventurous before I started my 30 day challenges. I also figured out that if you really want something badly enough, you can do anything for 30 days. Have you ever wondered a novel! Every November? tense of thousands of people try to write their own 50 , 000 word novel from scratch in 30 days. It turns out all you have to do is right. 1667 words a day for a month. So I did. By the way, the secret is not to go to sleep until you've written your words for the day. You might be sleep deprived, but you'll finish your novel. Now is my book the next Great American novel! No, I wrote it in a month. It's awful. But for the rest of my life, if I meet John Hodgman at a Ted party, I don't have to say I'm a computer scientist. No, no, if I want to, I can say I'm a novelist. So here's one last thing I like to mention. I learned that when I made small, sustainable changes, things I could keep doing there were more likely to stick. There's nothing wrong with big crazy challenges. In fact, they're a ton of fun. butter,less. likely to stick. When I gave up sugar for 30 days. Day 31 Look like this. So here's my question to you. What are you waiting for! I guarantee you the next 30 days are going to pass whether you like it or not. So why not think about something you have always wanted to try and give it a shot for the next 30 days. Thanks \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RestorePunctuation().restore(text=recognized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading small core english model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, few, years, ago, i, felt, like, i, was, stuck]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(recognized_text)\n",
    "list(doc)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a few years ago i felt like i was stuck in a rat so i decided to follow in the footsteps of the great american philosopher morgan spurlock and try something new for 30 days the idea is actually pretty simple think about something you've always wanted to add your life and try it for the next 30 days it turns out 30 days is just about the right amount of time to add a new habit or subtract the habit like watching the news from your life there's a few things that i learned while doing these 30 day challenges the first was instead of the months flying by forgotten the time was much more memorable this was part of a challenge i did to take a picture everyday for a month and i remember exactly where i was and what i was doing that day i also noticed that as i started to do more and harder 30 day challenges myself confidence grew i went from death dwelling computer nerd to the kind of guy who bikes to work for fun even last year i ended up hiking up mount kilimanjaro the highest mountain africa i would never have been that adventurous before i started my 30 day challenges i also figured out that if you really want something badly enough you can do anything for 30 days have you ever wondered a novel every november tense of thousands of people try to write their own 50,000 word novel from scratch in 30 days it turns out all you have to do is right 1667 words a day for a month\"\n",
      "\"so i did by the way the secret is not to go to sleep until you've written your words for the day you might be sleep deprived but you'll finish your novel now is my book the next great american novel\"\n",
      "\"no i wrote it in a month it's awful but for the rest of my life if i meet john hodgman at a ted party i don't have to say i'm a computer scientist\"\n",
      "\"no\"\n",
      "\"no if i want to i can say i'm a novelist so here's one last thing i like to mention i learned that when i made small sustainable changes things i could keep doing there were more likely to stick there's nothing wrong with big crazy challenges in fact they're a ton of fun butterless likely to stick when i gave up sugar for 30 days day 31 look like this\"\n",
      "\"so here's my question to you what are you waiting for i guarantee you the next 30 days are going to pass whether you like it or not so why not think about something you have always wanted to try and give it a shot for the next 30 days thanks\"\n"
     ]
    }
   ],
   "source": [
    "quoted_sentences = ['\"{}\"'.format(sent.text.strip()) for sent in doc.sents]\n",
    "\n",
    "for sentence in quoted_sentences:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the word and character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 483\n",
      "Character count: 2334\n"
     ]
    }
   ],
   "source": [
    "print(f'Word count: {len(doc)}')\n",
    "print(f'Character count: {len(recognized_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, WordList\n",
    "\n",
    "# Download nltk models and datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenizing the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'few', 'years', 'ago', 'i', 'felt', 'like', 'i', 'was', 'stuck']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_recognized_text = recognized_text.split(' ')\n",
    "tokenized_recognized_text[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the word and character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 471\n",
      "Character count: 2334\n"
     ]
    }
   ],
   "source": [
    "print(f'Word count: {len(tokenized_recognized_text)}')\n",
    "print(f'Character count: {len(recognized_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing words and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" years ago felt like stuck rat decided follow footsteps great american philosopher morgan spurlock try something new 30 days idea actually pretty simple think something always wanted add life try next 30 days turns 30 days right amount time add new habit subtract habit like watching news life there's things learned 30 day challenges first instead months flying forgotten time much memorable part challenge take picture everyday month remember exactly day also noticed started harder 30 day challenges confidence grew went death dwelling computer nerd kind guy bikes work fun even last year ended hiking mount kilimanjaro highest mountain africa would never adventurous started 30 day challenges also figured really want something badly enough anything 30 days ever wondered novel every november tense thousands people try write 50,000 word novel scratch 30 days turns right 1667 words day month way secret go sleep written words day might sleep deprived finish novel book next great american novel wrote month awful rest life meet john hodgman ted party say i'm computer scientist want say i'm novelist here's one last thing like mention learned made small sustainable changes things could keep likely stick there's nothing wrong big crazy challenges fact they're ton fun butterless likely stick gave sugar 30 days day 31 look like here's question waiting guarantee next 30 days going pass whether like think something always wanted try give shot next 30 days thanks\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "recognized_text_without_stopwords = ''\n",
    "recognized_text_stopwords = ''\n",
    "\n",
    "for word in tokenized_recognized_text:\n",
    "  if (word in english_stopwords):\n",
    "    recognized_text_stopwords += ' {}'.format(word)\n",
    "  else:\n",
    "    recognized_text_without_stopwords += ' {}'.format(word)\n",
    "\n",
    "recognized_text_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text: str) -> TextBlob:\n",
    "    tb_singularized_words = TextBlob(text).words.singularize()\n",
    "    return TextBlob(\" \".join(tb_singularized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"year ago felt like stuck rat decided follow footstep great american philosopher morgan spurlock try something new 30 day idea actually pretty simple think something alway wanted add life try next 30 day turn 30 day right amount time add new habit subtract habit like watching news life there ' thing learned 30 day challenge first instead month flying forgotten time much memorable part challenge take picture everyday month remember exactly day also noticed started harder 30 day challenge confidence grew went death dwelling computer nerd kind guy bike work fun even last year ended hiking mount kilimanjaro highest mountain africa would never adventurou started 30 day challenge also figured really want something badly enough anything 30 day ever wondered novel every november tense thousand person try write 50,000 word novel scratch 30 day turn right 1667 word day month way secret go sleep written word day might sleep deprived finish novel book next great american novel wrote month awful rest life meet john hodgman ted party say i 'm computer scientist want say i 'm novelist here ' one last thing like mention learned made small sustainable change thing could keep likely stick there ' nothing wrong big crazy challenge fact they 're ton fun butterles likely stick gave sugar 30 day day 31 look like here ' question waiting guarantee next 30 day going pas whether like think something alway wanted try give shot next 30 day thank\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_recognized_text_without_stopwords = lemmatize_words(recognized_text_without_stopwords)\n",
    "tb_recognized_text_stopwords = TextBlob(recognized_text_stopwords)\n",
    "\n",
    "tb_recognized_text_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatized recognized text and stopwords removed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 236\n",
      "Character count: 1440\n"
     ]
    }
   ],
   "source": [
    "print(f'Word count: {len(tb_recognized_text_without_stopwords.words)}')\n",
    "print(f'Character count: {len(tb_recognized_text_without_stopwords)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most popular words (lematized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day', 15),\n",
       " ('30', 11),\n",
       " ('like', 5),\n",
       " ('challenge', 5),\n",
       " ('try', 4),\n",
       " ('something', 4),\n",
       " ('next', 4),\n",
       " ('month', 4),\n",
       " ('novel', 4),\n",
       " ('life', 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(\n",
    "  tb_recognized_text_without_stopwords\n",
    "    .word_counts\n",
    "    .items(),\n",
    "  key=lambda item: item[1],\n",
    "  reverse=True\n",
    ")[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most popular stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 27),\n",
       " ('to', 19),\n",
       " ('the', 19),\n",
       " ('a', 16),\n",
       " ('you', 13),\n",
       " ('for', 11),\n",
       " ('of', 9),\n",
       " ('it', 7),\n",
       " ('was', 6),\n",
       " ('and', 6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(\n",
    "  tb_recognized_text_stopwords\n",
    "    .word_counts\n",
    "    .items(),\n",
    "  key=lambda item: item[1],\n",
    "  reverse=True\n",
    ")[0: 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
