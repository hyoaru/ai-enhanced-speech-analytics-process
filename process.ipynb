{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-enhanced Speech Analytics Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature checklist\n",
    "- [ ] Speech length\n",
    "- [ ] Words per minute\n",
    "- [ ] Recognized text\n",
    "- [ ] Word count\n",
    "- [ ] Character count\n",
    "- [ ] Stemming/lemmatization\n",
    "- [ ] Popular words and stopwords\n",
    "- [ ] Sentiment analysis\n",
    "- [ ] Parts of speech tagging\n",
    "- [ ] Text summarization\n",
    "- [ ] Spelling correction\n",
    "- [ ] Tagalog language support\n",
    "\n",
    "\n",
    "## Challenges\n",
    "* Filipino language\n",
    "* Code switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemeting speech recognition\n",
    "Implementing speech recognition using a TedEd 3 minutes talk titled 'Try something new for 30 days' by Matt Cutts as audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the audio file to wav format and feeding it to Google speech recognition engine to get the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized text: a few years ago i felt like i was stuck in a rat so i decided to follow in the footsteps of the great american philosopher morgan spurlock and try something new for 30 days the idea is actually pretty simple think about something you've always wanted to add your life and try it for the next 30 days it turns out 30 days is just about the right amount of time to add a new habit or subtract the habit like watching the news from your life there's a few things that i learned while doing these 30 day challenges the first was instead of the months flying by forgotten the time was much more memorable this was part of a challenge i did to take a picture everyday for a month and i remember exactly where i was and what i was doing that day i also noticed that as i started to do more and harder 30 day challenges myself confidence grew i went from death dwelling computer nerd to the kind of guy who bikes to work for fun even last year i ended up hiking up mount kilimanjaro the highest mountain africa i would never have been that adventurous before i started my 30 day challenges i also figured out that if you really want something badly enough you can do anything for 30 days have you ever wondered a novel every november tense of thousands of people try to write their own 50,000 word novel from scratch in 30 days it turns out all you have to do is right 1667 words a day for a month so i did by the way the secret is not to go to sleep until you've written your words for the day you might be sleep deprived but you'll finish your novel now is my book the next great american novel no i wrote it in a month it's awful but for the rest of my life if i meet john hodgman at a ted party i don't have to say i'm a computer scientist no no if i want to i can say i'm a novelist so here's one last thing i like to mention i learned that when i made small sustainable changes things i could keep doing there were more likely to stick there's nothing wrong with big crazy challenges in fact they're a ton of fun butterless likely to stick when i gave up sugar for 30 days day 31 look like this so here's my question to you what are you waiting for i guarantee you the next 30 days are going to pass whether you like it or not so why not think about something you have always wanted to try and give it a shot for the next 30 days thanks\n"
     ]
    }
   ],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "try:\n",
    "  audio_full_filename = 'TedEd - Try something new for 30 days.mp3'\n",
    "  audio_filename = audio_full_filename[0:audio_full_filename.rfind('.')]\n",
    "  raw_audios_directory = 'audios/raw'\n",
    "  converted_audios_directory = 'audios/converted'\n",
    "\n",
    "  # Converting the audio file to wav format\n",
    "  audio_file = AudioSegment.from_file(f'{raw_audios_directory}/{audio_full_filename}')\n",
    "  test = audio_file.export(f'{converted_audios_directory}/{audio_filename}.wav', format='wav')\n",
    "\n",
    "  # Feeding the wav audio to Google speech recognition engine\n",
    "  with sr.AudioFile(f'{converted_audios_directory}/{audio_filename}.wav') as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    recognized_text = recognizer.recognize_google(audio_data)\n",
    "    print(f'Recognized text: {recognized_text}')\n",
    "\n",
    "except sr.UnknownValueError():\n",
    "  recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT Restore Punctuation pretrained model for sentence boundary recognition and punctuation restoration\n",
    "The [BERT Restore Punctuation](https://huggingface.co/felflare/bert-restore-punctuation) model by [felflare](https://huggingface.co/felflare) is a pretrained transformer model designed to restore punctuation and capitalization in text. This model is based on the bert-base-uncased architecture and has been fine-tuned specifically for punctuation restoration on Yelp Reviews. It is highly effective for use cases such as automatic speech recognition (ASR) outputs or any other scenarios where text has lost its punctuation and capitalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestorePunctuation:\n",
    "  _LABEL_MAP = {\n",
    "    \"LABEL_0\": \"OU\",\n",
    "    \"LABEL_1\": \"OO\",\n",
    "    \"LABEL_2\": \".O\",\n",
    "    \"LABEL_3\": \"!O\",\n",
    "    \"LABEL_4\": \",O\",\n",
    "    \"LABEL_5\": \".U\",\n",
    "    \"LABEL_6\": \"!U\",\n",
    "    \"LABEL_7\": \",U\",\n",
    "    \"LABEL_8\": \":O\",\n",
    "    \"LABEL_9\": \";O\",\n",
    "    \"LABEL_10\": \":U\",\n",
    "    \"LABEL_11\": \"'O\",\n",
    "    \"LABEL_12\": \"-O\",\n",
    "    \"LABEL_13\": \"?O\",\n",
    "    \"LABEL_14\": \"?U\",\n",
    "  }\n",
    "\n",
    "  def __init__(self):\n",
    "    self._tokenizer = AutoTokenizer.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "    self._model = AutoModelForTokenClassification.from_pretrained(\"felflare/bert-restore-punctuation\")\n",
    "    self._pipe = pipeline('token-classification', model=self._model, tokenizer=self._tokenizer)\n",
    "\n",
    "  def restore(self, text: str):\n",
    "    predictions = self._pipe(text)\n",
    "    \n",
    "    restored_text = ''\n",
    "    for token_prediction in predictions:\n",
    "      label = self._LABEL_MAP[token_prediction['entity']]\n",
    "\n",
    "      if \"U\" in label:\n",
    "        restored_text += (token_prediction['word'].capitalize())\n",
    "      else:\n",
    "        restored_text += (token_prediction['word'])\n",
    "\n",
    "      for punctuation in [\".\", \",\", \"'\", \"-\", \":\", \";\", \"!\", \"?\"]:\n",
    "        if punctuation in label:\n",
    "          restored_text += punctuation\n",
    "       \n",
    "      restored_text += ' '\n",
    "\n",
    "    restored_text = (\n",
    "      restored_text\n",
    "      .replace(' ##', '')\n",
    "      .replace(\" ' \", \"'\"))\n",
    "      \n",
    "    return restored_text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A few years ago I felt like I was stuck in a rat. so I decided to follow in the footsteps of the great American philosopher Morgan Spurlock and try something new for 30 days. The idea is actually pretty simple. Think about something you've always wanted to add your life and try it for the next 30 days. It turns out 30 days is just about the right amount of time to add a new habit or subtract the habit. like watching the news from your life. There's a few things that I learned while doing these 30 day challenges. The first was instead of the months flying by forgotten, the time was much more memorable. This was part of a challenge I did to take a picture everyday for a month and I remember exactly where I was and what I was doing that day. I also noticed that as I started to do more and harder 30 day challenges myself, confidence grew. I went from death dwelling computer nerd to the kind of guy who bikes to work for fun. Even last year I ended up hiking up Mount Ki,limanjar,o, the highest mountain Africa. I would never have been that adventurous before I started my 30 day challenges. I also figured out that if you really want something badly enough, you can do anything for 30 days. Have you ever wondered a novel? Every November? tense of thousands of people try to write their own 50 , 000 word novel from scratch in 30 days. It turns out all you have to do is right. 1667 words a day for a month. So I did. By the way, the secret is not to go to sleep until you've written your words for the day. You might be sleep deprived, but you'll finish your novel. Now is my book the next Great American novel? No, I wrote it in a month. It's awful. But for the rest of my life, if I meet John Hodgman at a Ted party, I don't have to say I'm a computer scientist. No, no, if I want to, I can say I'm a novelist. So here's one last thing I like to mention. I learned that when I made small, sustainable changes, things I could keep doing there were more likely to stick. There's nothing wrong with big crazy challenges. In fact, they're a ton of fun. butter,less. likely to stick. When I gave up sugar for 30 days. Day 31 Look like this. So here's my question to you. What are you waiting for? I guarantee you the next 30 days are going to pass whether you like it or not. So why not think about something you have always wanted to try and give it a shot for the next 30 days. Thanks \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_text = RestorePunctuation().restore(text=recognized_text)\n",
    "restored_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarize:\n",
    "  def __init__(self):\n",
    "    self._tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "    self._model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")\n",
    "    self._pipe = pipeline('summarization', model=self._model, tokenizer=self._tokenizer)\n",
    "\n",
    "  def summarize(self, text:str):\n",
    "    summary = self._pipe(text)[0]['summary_text'].replace(' . ', '. ')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The idea is actually pretty simple. Think about something you've always wanted to add your life and try it for the next 30 days. It turns out 30 days is just about the right amount of time to add a new habit or subtract the habit. The first was instead of the months flying by forgotten, the time was much more memorable. I went from death dwelling computer nerd to the kind of guy who bikes to work for fun .\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextSummarize().summarize(restored_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading small core english model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the restored transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The \tLemma: the \tPOS: DET \tEntity: ORG\n",
      "Word: Big \tLemma: Big \tPOS: PROPN \tEntity: ORG\n",
      "Word: Brown \tLemma: Brown \tPOS: PROPN \tEntity: ORG\n",
      "Word: Fox \tLemma: Fox \tPOS: PROPN \tEntity: ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(restored_text)\n",
    "for token in list(doc)[:10]:\n",
    "  print(f\"Word: {token.text} \\tLemma: {token.lemma_} \\tPOS: {token.pos_} \\tEntity: {token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Big Brown Fox\n"
     ]
    }
   ],
   "source": [
    "for sentence in list(doc.sents)[:10]:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the token, word, and character count (pre-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_stats(doc):\n",
    "  token_count = len(doc)\n",
    "  character_count = len(restored_text)\n",
    "  word_count = len([token.text for token in doc if not token.is_punct and not token.is_space and not token.like_num])\n",
    "\n",
    "  return token_count, word_count, character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 4\n",
      "Word count: 4\n",
      "Character count: 18\n"
     ]
    }
   ],
   "source": [
    "token_count, word_count, character_count = get_count_stats(doc)\n",
    "\n",
    "print(f'Token count: {token_count}')\n",
    "print(f'Word count: {word_count}')\n",
    "print(f'Character count: {character_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_doc(doc):\n",
    "  lemmatized_text = \"\"\n",
    "  stop_words = []\n",
    "\n",
    "  for token in doc:\n",
    "    if token.is_punct:\n",
    "      continue\n",
    "\n",
    "    if token.is_stop:\n",
    "      stop_words.append(token.text.lower())\n",
    "    else:\n",
    "      lemmatized_text += f\"{token.lemma_} \"\n",
    "\n",
    "  return lemmatized_text, stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text, stop_words = standardize_doc(doc)\n",
    "doc_lemmatized = nlp(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Big \tLemma: Big \tPOS: PROPN \tEntity: ORG\n",
      "Word: Brown \tLemma: Brown \tPOS: PROPN \tEntity: ORG\n",
      "Word: Fox \tLemma: Fox \tPOS: PROPN \tEntity: ORG\n"
     ]
    }
   ],
   "source": [
    "for token in list(doc_lemmatized)[:10]:\n",
    "  print(f\"Word: {token.text} \\tLemma: {token.lemma_} \\tPOS: {token.pos_} \\tEntity: {token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the token, word, and character count (pre-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 3\n",
      "Word count: 3\n",
      "Character count: 18\n"
     ]
    }
   ],
   "source": [
    "token_count, word_count, character_count = get_count_stats(doc_lemmatized)\n",
    "\n",
    "print(f'Token count: {token_count}')\n",
    "print(f'Word count: {word_count}')\n",
    "print(f'Character count: {character_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(doc):\n",
    "  word_list = [token.text.lower() for token in doc if not token.is_punct and not token.is_space and not token.like_num]\n",
    "  return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most popular words (pre-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('big', 1), ('brown', 1), ('fox', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(get_word_list(doc)).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most popular words (standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('big', 1), ('brown', 1), ('fox', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(get_word_list(doc_lemmatized)).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most popular stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(stop_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
